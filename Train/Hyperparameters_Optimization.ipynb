{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Hyperparameters_Optimization.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "AK-YF0Xq7T7O",
        "i4Sa9Xpk7T7d",
        "yUpQXFku7T7j",
        "2-2OeSGt7T7o",
        "RO9rFd237T7t",
        "repC6M-g7T7x",
        "lQullSP27T71",
        "3OvKXj8Q7T75",
        "velppBYd7T8B",
        "NkbdS6xi7T8F",
        "YcSdRfJ47T8P",
        "o9LpI_wg7T8g",
        "SKJgXaTq7T8n",
        "98vZGUsWhRam",
        "DOw9DeK5hRar"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9V8mqxjS7T67",
        "colab": {}
      },
      "source": [
        "# Include all the necessary packages, \n",
        "import random\n",
        "import pandas as pd # pandas for data manipulation and analysis\n",
        "import numpy as np # numpy to handle multi-dimensional arrays and matricies\n",
        "import matplotlib # matplotlib for pixel plot\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JWFV9sr07T8o",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6f0UmeFH-hwb",
        "colab": {}
      },
      "source": [
        "#Include all the necessary keras libraries\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.losses import categorical_crossentropy\n",
        "from keras.optimizers import Adam\n",
        "from keras.regularizers import l2\n",
        "from keras.callbacks import ReduceLROnPlateau, TensorBoard, EarlyStopping, ModelCheckpoint\n",
        "from keras.models import load_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4LxbfouWhRZ-"
      },
      "source": [
        "# Hyperparameter optimalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P5yiuvd_84ti",
        "outputId": "036ef821-66d8-4127-d704-a73477038032",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip3 install hyperas\n",
        "!pip3 install hyperopt"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: hyperas in /usr/local/lib/python3.6/dist-packages (0.4.1)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from hyperas) (2.2.5)\n",
            "Requirement already satisfied: hyperopt in /usr/local/lib/python3.6/dist-packages (from hyperas) (0.1.2)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from hyperas) (5.6.1)\n",
            "Requirement already satisfied: jupyter in /usr/local/lib/python3.6/dist-packages (from hyperas) (1.0.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.6/dist-packages (from hyperas) (0.3)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.6/dist-packages (from hyperas) (4.4.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (1.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (2.8.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (1.17.4)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (1.3.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (3.13)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (1.0.8)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (1.12.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (4.28.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (2.4)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (3.10.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (0.16.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (2.1.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (3.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (0.6.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (4.3.3)\n",
            "Requirement already satisfied: jinja2>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (2.10.3)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (4.6.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (0.8.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (1.4.2)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (0.4.4)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (7.5.1)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (5.2.0)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (5.2.2)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (4.6.1)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (4.6.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from nbformat->hyperas) (0.2.0)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat->hyperas) (2.6.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->hyperopt->hyperas) (4.4.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->hyperas) (0.5.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.4->nbconvert->hyperas) (1.1.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->hyperas) (3.5.1)\n",
            "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->hyperas) (5.5.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->hyperas) (5.3.4)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->hyperas) (1.0.18)\n",
            "Requirement already satisfied: terminado>=0.3.3; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->hyperas) (0.8.3)\n",
            "Requirement already satisfied: tornado>=4 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->hyperas) (4.5.3)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter->hyperas) (42.0.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter->hyperas) (0.7.5)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter->hyperas) (0.8.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter->hyperas) (4.7.0)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->jupyter-console->jupyter->hyperas) (17.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->jupyter-console->jupyter->hyperas) (2.6.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.0->jupyter-console->jupyter->hyperas) (0.1.7)\n",
            "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.3.3; sys_platform != \"win32\"->notebook->jupyter->hyperas) (0.6.0)\n",
            "Requirement already satisfied: hyperopt in /usr/local/lib/python3.6/dist-packages (0.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from hyperopt) (4.28.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt) (2.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from hyperopt) (1.3.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from hyperopt) (1.17.4)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt) (3.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from hyperopt) (1.12.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->hyperopt) (4.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3ZDv2Ry984tk",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.callbacks import EarlyStopping\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tzdPnsGJ84tm",
        "colab": {}
      },
      "source": [
        "def data():\n",
        "    df = pd.read_csv('fer2013.csv')\n",
        "    # Convert the pixel column values into a Python list object\n",
        "    pixels = df['pixels'].tolist()\n",
        "    emotions = df['emotion'].tolist()\n",
        "\n",
        "    # The fer2013 database contains 48x48 face images so we create two variables \n",
        "    # to store the width and the height of the image\n",
        "    width, height = 48, 48\n",
        "\n",
        "    # Convert each pixel set (pixel array) to a 48x48 image and\n",
        "    # create a list called faces to store each face image\n",
        "    faces = []\n",
        "    for pixel_sequence in pixels:\n",
        "        # Use Python's list comprehension because it's quicker than a single for cycle\n",
        "        face = [int(pixel) for pixel in pixel_sequence.split(' ')]\n",
        "        # Reshape face array to matches the 48x48 face images\n",
        "        face = np.asarray(face).reshape(width, height)\n",
        "        # Add the converted face image to the faces list\n",
        "        faces.append(face.astype('uint8'))\n",
        "\n",
        "    # Convert the list to a numpy array\n",
        "    faces = np.asarray(faces)\n",
        "    emotions = np.asarray(emotions)\n",
        "\n",
        "    # Expanding the dimension of channel for each image\n",
        "    faces_exp = np.expand_dims(faces, -1)\n",
        "    # Converting the labels to catergorical matrix\n",
        "    emotions_categori = pd.get_dummies(df['emotion']).as_matrix() \n",
        "\n",
        "    # Create a dictionary for identify the emotion\n",
        "    emotion_dict = {0:\"Angry\", 1:\"Disgust\", 2:\"Fear\", 3:\"Happy\", 4:\"Sad\", 5:\"Surprise\", 6:\"Neutral\"}\n",
        "\n",
        "    # Split the train data into a train and validation group (validation = 20% of the train data)\n",
        "    x_train, x_val, y_train, y_val = train_test_split(faces_exp, emotions_categori, test_size=0.2, random_state=30)\n",
        "    # Split the train data into a train and test group (test = 10% of the original train data)\n",
        "    x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.125, random_state=30)\n",
        "\n",
        "    return x_train, y_train, x_test, y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ytq3gBgS84to",
        "colab": {}
      },
      "source": [
        "def create_model(x_train,y_train,x_test,y_test):\n",
        "    from keras.layers import Layer\n",
        "    from keras import backend as K\n",
        "    from keras.layers import LeakyReLU\n",
        "\n",
        "    num_features = 64 \n",
        "    num_labels = 7 # Seven different emotions\n",
        "    batch_size = 64 # One batch contains 64 images\n",
        "    epochs = 30\n",
        "    width, height = 48, 48  # Image size\n",
        " \n",
        "    n_layer = {{choice([16, 32, 64])}}\n",
        "    dropout_n = {{uniform(0, 0.5)}}\n",
        "    dropout_2 = {{uniform(0, 0.5)}}\n",
        "    act = {{choice(['relu', 'linear', 'tanh'])}}\n",
        "    optim = {{choice(['rmsprop', 'adam', 'sgd'])}}\n",
        "    n_batch = {{choice([64, 128, 256])}}\n",
        "    print('a modell hiperparaméterei: ', n_layer, dropout_n, dropout_2, act, optim, n_batch)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(n_layer, kernel_size=(3, 3), activation = act, input_shape=(width, height, 1), data_format='channels_last', kernel_regularizer=l2(0.01)))\n",
        "    model.add(Conv2D(n_layer, kernel_size=(3, 3), activation = act, padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "    model.add(Dropout(dropout_n))\n",
        "\n",
        "    model.add(Conv2D(2*n_layer, kernel_size=(3, 3), activation = act, padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(2*n_layer, kernel_size=(3, 3), activation = act, padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "    model.add(Dropout(dropout_n))\n",
        "\n",
        "    model.add(Conv2D(2*2*n_layer, kernel_size=(3, 3), activation = act, padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(2*2*n_layer, kernel_size=(3, 3), activation = act, padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "    model.add(Dropout(dropout_n))\n",
        "\n",
        "    model.add(Conv2D(2*2*2*n_layer, kernel_size=(3, 3), activation = act, padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(2*2*2*n_layer, kernel_size=(3, 3), activation = act, padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "    model.add(Dropout(dropout_n))\n",
        "\n",
        "    # Transform the Inputs to row vector\n",
        "    model.add(Flatten())\n",
        "              \n",
        "    # With dense, we define the fully-connected layers \n",
        "    model.add(Dense(2*2*2*n_layer, activation = act))\n",
        "    model.add(Dropout(dropout_2))\n",
        "    model.add(Dense(2*2*n_layer, activation = act))\n",
        "    model.add(Dropout(dropout_2))\n",
        "    model.add(Dense(2*n_layer, activation = act))\n",
        "    model.add(Dropout(dropout_n))\n",
        "    \n",
        "    model.add(Dense(num_labels, activation='softmax'))\n",
        "\n",
        "    model.compile(loss = categorical_crossentropy,\n",
        "                  optimizer = optim,\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    lr_reducer = ReduceLROnPlateau(monitor='val_acc', factor=0.9, patience=8, verbose=1)\n",
        "\n",
        "    early_stopper = EarlyStopping(monitor='val_acc', min_delta=0, patience=15, verbose=0, mode='auto')\n",
        "\n",
        "    checkpointer = ModelCheckpoint('model.h5', monitor='val_acc', verbose=1, save_best_only=True)\n",
        "\n",
        "    result = model.fit(np.array(x_train), np.array(y_train),\n",
        "              batch_size = n_batch,\n",
        "              epochs = epochs,\n",
        "              verbose = 2,\n",
        "              validation_data = (np.array(x_val), np.array(y_val)),\n",
        "              shuffle = True,\n",
        "              callbacks = [lr_reducer, early_stopper, checkpointer])\n",
        "\n",
        "    \n",
        "    # az epoch-ok közül a legnagyobb val_acc elmentése\n",
        "    best_val_acc = np.amax(result.history['val_acc']) \n",
        "    print('legjobb val_acc:', best_val_acc)  \n",
        "    \n",
        "    # log kiírása: háló struktúra, és az eredmény\n",
        "    with open('emotionrecopt.csv', 'a') as csv_file:\n",
        "      csv_file.write(str(n_layer) + ';')\n",
        "      csv_file.write(str(dropout_n) + ';')\n",
        "      csv_file.write(str(act) + ';')\n",
        "      csv_file.write(str(optim) + ';')\n",
        "      csv_file.write(str(n_batch) + ';')\n",
        "      csv_file.write(str(best_val_acc) + '\\n')\n",
        "\n",
        "\n",
        "    # negatív val_acc, mert a hyperopt csomag mindig minimalizál\n",
        "    return {'loss': -best_val_acc, 'status': STATUS_OK, 'model': model}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bavDuAKi84tr",
        "colab": {}
      },
      "source": [
        "import hyperas\n",
        "from hyperopt import Trials, STATUS_OK, tpe\n",
        "from hyperas import optim\n",
        "from hyperas.distributions import choice,uniform"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CCVSxxV284tv",
        "outputId": "faf34538-15f7-4427-e2fc-b8d0e4d33fd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "best_run, best_model = optim.minimize(model=create_model,\n",
        "                                          data=data,\n",
        "                                          algo=tpe.suggest,\n",
        "                                          max_evals=5,\n",
        "                                          notebook_name='Hyperparameters_Optimization',\n",
        "                                          trials=Trials())"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">>> Imports:\n",
            "#coding=utf-8\n",
            "\n",
            "try:\n",
            "    import random\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import pandas as pd\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import numpy as np\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import matplotlib\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import matplotlib.pyplot as plt\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from sklearn.model_selection import train_test_split\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.models import Sequential\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.layers import Dense, Dropout, Activation, Flatten\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.losses import categorical_crossentropy\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.optimizers import Adam\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.regularizers import l2\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.callbacks import ReduceLROnPlateau, TensorBoard, EarlyStopping, ModelCheckpoint\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.models import load_model\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import keras\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.datasets import fashion_mnist\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.models import Sequential\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.layers import Dense, Dropout, Activation\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.callbacks import EarlyStopping\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import numpy as np\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.layers import Layer\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras import backend as K\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.layers import LeakyReLU\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import hyperas\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from hyperopt import Trials, STATUS_OK, tpe\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from hyperas import optim\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from hyperas.distributions import choice, uniform\n",
            "except:\n",
            "    pass\n",
            "\n",
            ">>> Hyperas search space:\n",
            "\n",
            "def get_space():\n",
            "    return {\n",
            "        'n_layer': hp.choice('n_layer', [16, 32, 64]),\n",
            "        'dropout_n': hp.uniform('dropout_n', 0, 0.5),\n",
            "        'dropout_n_1': hp.uniform('dropout_n_1', 0, 0.5),\n",
            "        'act': hp.choice('act', ['relu', 'linear', 'tanh']),\n",
            "        'optim': hp.choice('optim', ['rmsprop', 'adam', 'sgd']),\n",
            "        'n_batch': hp.choice('n_batch', [64, 128, 256]),\n",
            "    }\n",
            "\n",
            ">>> Data\n",
            "   1: \n",
            "   2: df = pd.read_csv('fer2013.csv')\n",
            "   3: # Convert the pixel column values into a Python list object\n",
            "   4: pixels = df['pixels'].tolist()\n",
            "   5: emotions = df['emotion'].tolist()\n",
            "   6: \n",
            "   7: # The fer2013 database contains 48x48 face images so we create two variables \n",
            "   8: # to store the width and the height of the image\n",
            "   9: width, height = 48, 48\n",
            "  10: \n",
            "  11: # Convert each pixel set (pixel array) to a 48x48 image and\n",
            "  12: # create a list called faces to store each face image\n",
            "  13: faces = []\n",
            "  14: for pixel_sequence in pixels:\n",
            "  15:     # Use Python's list comprehension because it's quicker than a single for cycle\n",
            "  16:     face = [int(pixel) for pixel in pixel_sequence.split(' ')]\n",
            "  17:     # Reshape face array to matches the 48x48 face images\n",
            "  18:     face = np.asarray(face).reshape(width, height)\n",
            "  19:     # Add the converted face image to the faces list\n",
            "  20:     faces.append(face.astype('uint8'))\n",
            "  21: \n",
            "  22: # Convert the list to a numpy array\n",
            "  23: faces = np.asarray(faces)\n",
            "  24: emotions = np.asarray(emotions)\n",
            "  25: \n",
            "  26: # Expanding the dimension of channel for each image\n",
            "  27: faces_exp = np.expand_dims(faces, -1)\n",
            "  28: # Converting the labels to catergorical matrix\n",
            "  29: emotions_categori = pd.get_dummies(df['emotion']).as_matrix() \n",
            "  30: \n",
            "  31: # Create a dictionary for identify the emotion\n",
            "  32: emotion_dict = {0:\"Angry\", 1:\"Disgust\", 2:\"Fear\", 3:\"Happy\", 4:\"Sad\", 5:\"Surprise\", 6:\"Neutral\"}\n",
            "  33: \n",
            "  34: # Split the train data into a train and validation group (validation = 20% of the train data)\n",
            "  35: x_train, x_val, y_train, y_val = train_test_split(faces_exp, emotions_categori, test_size=0.2, random_state=30)\n",
            "  36: # Split the train data into a train and test group (test = 10% of the original train data)\n",
            "  37: x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.125, random_state=30)\n",
            "  38: \n",
            "  39: \n",
            "  40: \n",
            "  41: \n",
            ">>> Resulting replaced keras model:\n",
            "\n",
            "   1: def keras_fmin_fnct(space):\n",
            "   2: \n",
            "   3: \n",
            "   4:     num_features = 64 \n",
            "   5:     num_labels = 7 # Seven different emotions\n",
            "   6:     batch_size = 64 # One batch contains 64 images\n",
            "   7:     epochs = 30\n",
            "   8:     width, height = 48, 48  # Image size\n",
            "   9:  \n",
            "  10:     n_layer = space['n_layer']\n",
            "  11:     dropout_n = space['dropout_n']\n",
            "  12:     dropout_2 = space['dropout_n_1']\n",
            "  13:     act = space['act']\n",
            "  14:     optim = space['optim']\n",
            "  15:     n_batch = space['n_batch']\n",
            "  16:     print('a modell hiperparaméterei: ', n_layer, dropout_n, dropout_2, act, optim, n_batch)\n",
            "  17: \n",
            "  18:     model = Sequential()\n",
            "  19:     model.add(Conv2D(n_layer, kernel_size=(3, 3), activation = act, input_shape=(width, height, 1), data_format='channels_last', kernel_regularizer=l2(0.01)))\n",
            "  20:     model.add(Conv2D(n_layer, kernel_size=(3, 3), activation = act, padding='same'))\n",
            "  21:     model.add(BatchNormalization())\n",
            "  22:     model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
            "  23:     model.add(Dropout(dropout_n))\n",
            "  24: \n",
            "  25:     model.add(Conv2D(2*n_layer, kernel_size=(3, 3), activation = act, padding='same'))\n",
            "  26:     model.add(BatchNormalization())\n",
            "  27:     model.add(Conv2D(2*n_layer, kernel_size=(3, 3), activation = act, padding='same'))\n",
            "  28:     model.add(BatchNormalization())\n",
            "  29:     model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
            "  30:     model.add(Dropout(dropout_n))\n",
            "  31: \n",
            "  32:     model.add(Conv2D(2*2*n_layer, kernel_size=(3, 3), activation = act, padding='same'))\n",
            "  33:     model.add(BatchNormalization())\n",
            "  34:     model.add(Conv2D(2*2*n_layer, kernel_size=(3, 3), activation = act, padding='same'))\n",
            "  35:     model.add(BatchNormalization())\n",
            "  36:     model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
            "  37:     model.add(Dropout(dropout_n))\n",
            "  38: \n",
            "  39:     model.add(Conv2D(2*2*2*n_layer, kernel_size=(3, 3), activation = act, padding='same'))\n",
            "  40:     model.add(BatchNormalization())\n",
            "  41:     model.add(Conv2D(2*2*2*n_layer, kernel_size=(3, 3), activation = act, padding='same'))\n",
            "  42:     model.add(BatchNormalization())\n",
            "  43:     model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
            "  44:     model.add(Dropout(dropout_n))\n",
            "  45: \n",
            "  46:     # Transform the Inputs to row vector\n",
            "  47:     model.add(Flatten())\n",
            "  48:               \n",
            "  49:     # With dense, we define the fully-connected layers \n",
            "  50:     model.add(Dense(2*2*2*n_layer, activation = act))\n",
            "  51:     model.add(Dropout(dropout_2))\n",
            "  52:     model.add(Dense(2*2*n_layer, activation = act))\n",
            "  53:     model.add(Dropout(dropout_2))\n",
            "  54:     model.add(Dense(2*n_layer, activation = act))\n",
            "  55:     model.add(Dropout(dropout_n))\n",
            "  56:     \n",
            "  57:     model.add(Dense(num_labels, activation='softmax'))\n",
            "  58: \n",
            "  59:     model.compile(loss = categorical_crossentropy,\n",
            "  60:                   optimizer = optim,\n",
            "  61:                   metrics=['accuracy'])\n",
            "  62: \n",
            "  63:     lr_reducer = ReduceLROnPlateau(monitor='val_acc', factor=0.9, patience=8, verbose=1)\n",
            "  64: \n",
            "  65:     early_stopper = EarlyStopping(monitor='val_acc', min_delta=0, patience=15, verbose=0, mode='auto')\n",
            "  66: \n",
            "  67:     checkpointer = ModelCheckpoint('model.h5', monitor='val_acc', verbose=1, save_best_only=True)\n",
            "  68: \n",
            "  69:     result = model.fit(np.array(x_train), np.array(y_train),\n",
            "  70:               batch_size = n_batch,\n",
            "  71:               epochs = epochs,\n",
            "  72:               verbose = 2,\n",
            "  73:               validation_data = (np.array(x_val), np.array(y_val)),\n",
            "  74:               shuffle = True,\n",
            "  75:               callbacks = [lr_reducer, early_stopper, checkpointer])\n",
            "  76: \n",
            "  77:     \n",
            "  78:     # az epoch-ok közül a legnagyobb val_acc elmentése\n",
            "  79:     best_val_acc = np.amax(result.history['val_acc']) \n",
            "  80:     print('legjobb val_acc:', best_val_acc)  \n",
            "  81:     \n",
            "  82:     # log kiírása: háló struktúra, és az eredmény\n",
            "  83:     with open('emotionrecopt.csv', 'a') as csv_file:\n",
            "  84:       csv_file.write(str(n_layer) + ';')\n",
            "  85:       csv_file.write(str(dropout_n) + ';')\n",
            "  86:       csv_file.write(str(act) + ';')\n",
            "  87:       csv_file.write(str(optim) + ';')\n",
            "  88:       csv_file.write(str(n_batch) + ';')\n",
            "  89:       csv_file.write(str(best_val_acc) + '\\n')\n",
            "  90: \n",
            "  91: \n",
            "  92:     # negatív val_acc, mert a hyperopt csomag mindig minimalizál\n",
            "  93:     return {'loss': -best_val_acc, 'status': STATUS_OK, 'model': model}\n",
            "  94: \n",
            "a modell hiperparaméterei: \n",
            "32\n",
            "0.3671073489296299\n",
            "0.3462695171578595\n",
            "linear\n",
            "sgd\n",
            "64\n",
            "  0%|          | 0/5 [00:00<?, ?it/s, best loss: ?]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/temp_model.py:166: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
            "  otions_categori = pd.get_dummies(df['emotion']).as_matrix()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 25120 samples, validate on 7178 samples\n",
            "Epoch 1/30\n",
            " - 9s - loss: 2.7326 - acc: 0.1985 - val_loss: 1.8171 - val_acc: 0.2485\n",
            "\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.24854, saving model to model.h5\n",
            "Epoch 2/30\n",
            " - 7s - loss: 1.8626 - acc: 0.2381 - val_loss: 1.7964 - val_acc: 0.2469\n",
            "\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.24854\n",
            "Epoch 3/30\n",
            " - 6s - loss: 1.8327 - acc: 0.2484 - val_loss: 1.7859 - val_acc: 0.2485\n",
            "\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.24854\n",
            "Epoch 4/30\n",
            " - 7s - loss: 1.8139 - acc: 0.2583 - val_loss: 1.7799 - val_acc: 0.2527\n",
            "\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.24854 to 0.25272, saving model to model.h5\n",
            "Epoch 5/30\n",
            " - 7s - loss: 1.7941 - acc: 0.2709 - val_loss: 1.7508 - val_acc: 0.2768\n",
            "\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.25272 to 0.27682, saving model to model.h5\n",
            "Epoch 6/30\n",
            " - 7s - loss: 1.7714 - acc: 0.2860 - val_loss: 1.7651 - val_acc: 0.2814\n",
            "\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.27682 to 0.28142, saving model to model.h5\n",
            "Epoch 7/30\n",
            " - 6s - loss: 1.7469 - acc: 0.3025 - val_loss: 1.7062 - val_acc: 0.3161\n",
            "\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.28142 to 0.31610, saving model to model.h5\n",
            "Epoch 8/30\n",
            " - 7s - loss: 1.7173 - acc: 0.3149 - val_loss: 1.7210 - val_acc: 0.3001\n",
            "\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.31610\n",
            "Epoch 9/30\n",
            " - 7s - loss: 1.6897 - acc: 0.3339 - val_loss: 1.6585 - val_acc: 0.3495\n",
            "\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.31610 to 0.34954, saving model to model.h5\n",
            "Epoch 10/30\n",
            " - 7s - loss: 1.6606 - acc: 0.3424 - val_loss: 1.6686 - val_acc: 0.3437\n",
            "\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.34954\n",
            "Epoch 11/30\n",
            " - 6s - loss: 1.6411 - acc: 0.3541 - val_loss: 1.6158 - val_acc: 0.3384\n",
            "\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.34954\n",
            "Epoch 12/30\n",
            " - 6s - loss: 1.6187 - acc: 0.3647 - val_loss: 1.6019 - val_acc: 0.3529\n",
            "\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.34954 to 0.35288, saving model to model.h5\n",
            "Epoch 13/30\n",
            " - 7s - loss: 1.6102 - acc: 0.3689 - val_loss: 1.5967 - val_acc: 0.3667\n",
            "\n",
            "\n",
            "Epoch 00013: val_acc improved from 0.35288 to 0.36668, saving model to model.h5\n",
            "Epoch 14/30\n",
            " - 6s - loss: 1.5957 - acc: 0.3798 - val_loss: 1.6158 - val_acc: 0.3717\n",
            "\n",
            "\n",
            "Epoch 00014: val_acc improved from 0.36668 to 0.37169, saving model to model.h5\n",
            "Epoch 15/30\n",
            " - 7s - loss: 1.5821 - acc: 0.3837 - val_loss: 1.4948 - val_acc: 0.4138\n",
            "\n",
            "\n",
            "Epoch 00015: val_acc improved from 0.37169 to 0.41376, saving model to model.h5\n",
            "Epoch 16/30\n",
            " - 7s - loss: 1.5659 - acc: 0.3897 - val_loss: 1.4929 - val_acc: 0.4221\n",
            "\n",
            "\n",
            "Epoch 00016: val_acc improved from 0.41376 to 0.42212, saving model to model.h5\n",
            "Epoch 17/30\n",
            " - 6s - loss: 1.5502 - acc: 0.3995 - val_loss: 1.6278 - val_acc: 0.3674\n",
            "\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.42212\n",
            "Epoch 18/30\n",
            " - 6s - loss: 1.5373 - acc: 0.4019 - val_loss: 1.7027 - val_acc: 0.3370\n",
            "\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.42212\n",
            "Epoch 19/30\n",
            " - 6s - loss: 1.5232 - acc: 0.4129 - val_loss: 1.4568 - val_acc: 0.4322\n",
            "\n",
            "\n",
            "Epoch 00019: val_acc improved from 0.42212 to 0.43215, saving model to model.h5\n",
            "Epoch 20/30\n",
            " - 7s - loss: 1.5100 - acc: 0.4151 - val_loss: 1.3920 - val_acc: 0.4685\n",
            "\n",
            "\n",
            "Epoch 00020: val_acc improved from 0.43215 to 0.46851, saving model to model.h5\n",
            "Epoch 21/30\n",
            " - 7s - loss: 1.4939 - acc: 0.4259 - val_loss: 1.4338 - val_acc: 0.4497\n",
            "\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.46851\n",
            "Epoch 22/30\n",
            " - 6s - loss: 1.4829 - acc: 0.4279 - val_loss: 1.4194 - val_acc: 0.4494\n",
            "\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.46851\n",
            "Epoch 23/30\n",
            " - 7s - loss: 1.4657 - acc: 0.4418 - val_loss: 1.3734 - val_acc: 0.4706\n",
            "\n",
            "\n",
            "Epoch 00023: val_acc improved from 0.46851 to 0.47060, saving model to model.h5\n",
            "Epoch 24/30\n",
            " - 7s - loss: 1.4556 - acc: 0.4428 - val_loss: 1.3527 - val_acc: 0.4916\n",
            "\n",
            "\n",
            "Epoch 00024: val_acc improved from 0.47060 to 0.49164, saving model to model.h5\n",
            "Epoch 25/30\n",
            " - 7s - loss: 1.4499 - acc: 0.4457 - val_loss: 1.3490 - val_acc: 0.4854\n",
            "\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.49164\n",
            "Epoch 26/30\n",
            " - 6s - loss: 1.4348 - acc: 0.4509 - val_loss: 1.3600 - val_acc: 0.4759\n",
            "\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.49164\n",
            "Epoch 27/30\n",
            " - 7s - loss: 1.4271 - acc: 0.4563 - val_loss: 1.3509 - val_acc: 0.4805\n",
            "\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.49164\n",
            "Epoch 28/30\n",
            " - 7s - loss: 1.4180 - acc: 0.4613 - val_loss: 1.3171 - val_acc: 0.4992\n",
            "\n",
            "\n",
            "Epoch 00028: val_acc improved from 0.49164 to 0.49916, saving model to model.h5\n",
            "Epoch 29/30\n",
            " - 7s - loss: 1.4115 - acc: 0.4605 - val_loss: 1.3424 - val_acc: 0.4877\n",
            "\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.49916\n",
            "Epoch 30/30\n",
            " - 7s - loss: 1.4060 - acc: 0.4638 - val_loss: 1.3639 - val_acc: 0.4721\n",
            "\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.49916\n",
            "legjobb val_acc:\n",
            "0.4991641125993896\n",
            "a modell hiperparaméterei: \n",
            "64\n",
            "0.25764043248072455\n",
            "0.21261430843422813\n",
            "tanh\n",
            "rmsprop\n",
            "128\n",
            "Train on 25120 samples, validate on 7178 samples\n",
            "Epoch 1/30\n",
            " - 14s - loss: 2.0562 - acc: 0.1992 - val_loss: 1.9620 - val_acc: 0.1868\n",
            "\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.18682, saving model to model.h5\n",
            "Epoch 2/30\n",
            " - 11s - loss: 1.9085 - acc: 0.2281 - val_loss: 1.8759 - val_acc: 0.2633\n",
            "\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.18682 to 0.26330, saving model to model.h5\n",
            "Epoch 3/30\n",
            " - 11s - loss: 1.7627 - acc: 0.2914 - val_loss: 1.9912 - val_acc: 0.1712\n",
            "\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.26330\n",
            "Epoch 4/30\n",
            " - 11s - loss: 1.5784 - acc: 0.3849 - val_loss: 2.2240 - val_acc: 0.1712\n",
            "\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.26330\n",
            "Epoch 5/30\n",
            " - 11s - loss: 1.4842 - acc: 0.4233 - val_loss: 2.3700 - val_acc: 0.1712\n",
            "\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.26330\n",
            "Epoch 6/30\n",
            " - 11s - loss: 1.4208 - acc: 0.4534 - val_loss: 2.2702 - val_acc: 0.1712\n",
            "\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.26330\n",
            "Epoch 7/30\n",
            " - 11s - loss: 1.3754 - acc: 0.4705 - val_loss: 2.3636 - val_acc: 0.1712\n",
            "\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.26330\n",
            "Epoch 8/30\n",
            " - 11s - loss: 1.3303 - acc: 0.4902 - val_loss: 2.3360 - val_acc: 0.1716\n",
            "\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.26330\n",
            "Epoch 9/30\n",
            " - 11s - loss: 1.3062 - acc: 0.5012 - val_loss: 2.5690 - val_acc: 0.1712\n",
            "\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.26330\n",
            "Epoch 10/30\n",
            " - 11s - loss: 1.2719 - acc: 0.5198 - val_loss: 2.5032 - val_acc: 0.1460\n",
            "\n",
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009000000427477062.\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.26330\n",
            "Epoch 11/30\n",
            " - 11s - loss: 1.2369 - acc: 0.5357 - val_loss: 1.7615 - val_acc: 0.2917\n",
            "\n",
            "\n",
            "Epoch 00011: val_acc improved from 0.26330 to 0.29172, saving model to model.h5\n",
            "Epoch 12/30\n",
            " - 11s - loss: 1.2170 - acc: 0.5432 - val_loss: 2.1506 - val_acc: 0.1910\n",
            "\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.29172\n",
            "Epoch 13/30\n",
            " - 11s - loss: 1.2061 - acc: 0.5439 - val_loss: 1.7200 - val_acc: 0.3215\n",
            "\n",
            "\n",
            "Epoch 00013: val_acc improved from 0.29172 to 0.32154, saving model to model.h5\n",
            "Epoch 14/30\n",
            " - 11s - loss: 1.1898 - acc: 0.5549 - val_loss: 2.0255 - val_acc: 0.2562\n",
            "\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.32154\n",
            "Epoch 15/30\n",
            " - 11s - loss: 1.1726 - acc: 0.5623 - val_loss: 2.1535 - val_acc: 0.2417\n",
            "\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.32154\n",
            "Epoch 16/30\n",
            " - 11s - loss: 1.1703 - acc: 0.5590 - val_loss: 2.5149 - val_acc: 0.1739\n",
            "\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.32154\n",
            "Epoch 17/30\n",
            " - 11s - loss: 1.1709 - acc: 0.5567 - val_loss: 2.4497 - val_acc: 0.1381\n",
            "\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.32154\n",
            "Epoch 18/30\n",
            " - 11s - loss: 1.1644 - acc: 0.5625 - val_loss: 1.8442 - val_acc: 0.2899\n",
            "\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.32154\n",
            "Epoch 19/30\n",
            " - 11s - loss: 1.1558 - acc: 0.5671 - val_loss: 1.6811 - val_acc: 0.3420\n",
            "\n",
            "\n",
            "Epoch 00019: val_acc improved from 0.32154 to 0.34202, saving model to model.h5\n",
            "Epoch 20/30\n",
            " - 11s - loss: 1.1522 - acc: 0.5685 - val_loss: 2.2245 - val_acc: 0.1843\n",
            "\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.34202\n",
            "Epoch 21/30\n",
            " - 11s - loss: 1.1420 - acc: 0.5741 - val_loss: 1.9988 - val_acc: 0.2423\n",
            "\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.34202\n",
            "Epoch 22/30\n",
            " - 11s - loss: 1.1367 - acc: 0.5738 - val_loss: 2.2099 - val_acc: 0.1794\n",
            "\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.34202\n",
            "Epoch 23/30\n",
            " - 11s - loss: 1.1502 - acc: 0.5720 - val_loss: 1.8041 - val_acc: 0.3210\n",
            "\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.34202\n",
            "Epoch 24/30\n",
            " - 11s - loss: 1.1462 - acc: 0.5705 - val_loss: 1.6568 - val_acc: 0.4135\n",
            "\n",
            "\n",
            "Epoch 00024: val_acc improved from 0.34202 to 0.41349, saving model to model.h5\n",
            "Epoch 25/30\n",
            " - 11s - loss: 1.1296 - acc: 0.5799 - val_loss: 2.0586 - val_acc: 0.2356\n",
            "\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.41349\n",
            "Epoch 26/30\n",
            " - 11s - loss: 1.1344 - acc: 0.5771 - val_loss: 2.1003 - val_acc: 0.2177\n",
            "\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.41349\n",
            "Epoch 27/30\n",
            " - 11s - loss: 1.1295 - acc: 0.5772 - val_loss: 2.2705 - val_acc: 0.1566\n",
            "\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.41349\n",
            "Epoch 28/30\n",
            " - 11s - loss: 1.1313 - acc: 0.5758 - val_loss: 2.8168 - val_acc: 0.1513\n",
            "\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.41349\n",
            "Epoch 29/30\n",
            " - 11s - loss: 1.1288 - acc: 0.5796 - val_loss: 2.5583 - val_acc: 0.1640\n",
            "\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.41349\n",
            "Epoch 30/30\n",
            " - 11s - loss: 1.1263 - acc: 0.5823 - val_loss: 2.7447 - val_acc: 0.1382\n",
            "\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.41349\n",
            "legjobb val_acc:\n",
            "0.4134856506156602\n",
            "a modell hiperparaméterei: \n",
            "64\n",
            "0.23634271098298437\n",
            "0.3637790828042422\n",
            "tanh\n",
            "adam\n",
            "256\n",
            "Train on 25120 samples, validate on 7178 samples\n",
            "Epoch 1/30\n",
            " - 14s - loss: 2.0642 - acc: 0.1886 - val_loss: 1.8396 - val_acc: 0.2458\n",
            "\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.24575, saving model to model.h5\n",
            "Epoch 2/30\n",
            " - 10s - loss: 1.9575 - acc: 0.2037 - val_loss: 1.8904 - val_acc: 0.1382\n",
            "\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.24575\n",
            "Epoch 3/30\n",
            " - 10s - loss: 1.9048 - acc: 0.2079 - val_loss: 2.0036 - val_acc: 0.2462\n",
            "\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.24575 to 0.24617, saving model to model.h5\n",
            "Epoch 4/30\n",
            " - 10s - loss: 1.7562 - acc: 0.2977 - val_loss: 1.9023 - val_acc: 0.1610\n",
            "\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.24617\n",
            "Epoch 5/30\n",
            " - 10s - loss: 1.6259 - acc: 0.3563 - val_loss: 2.0717 - val_acc: 0.1725\n",
            "\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.24617\n",
            "Epoch 6/30\n",
            " - 10s - loss: 1.5257 - acc: 0.4001 - val_loss: 2.0760 - val_acc: 0.2590\n",
            "\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.24617 to 0.25899, saving model to model.h5\n",
            "Epoch 7/30\n",
            " - 10s - loss: 1.4506 - acc: 0.4376 - val_loss: 2.3885 - val_acc: 0.2462\n",
            "\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.25899\n",
            "Epoch 8/30\n",
            " - 10s - loss: 1.4005 - acc: 0.4599 - val_loss: 2.0371 - val_acc: 0.2416\n",
            "\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.25899\n",
            "Epoch 9/30\n",
            " - 10s - loss: 1.3532 - acc: 0.4766 - val_loss: 2.5613 - val_acc: 0.1712\n",
            "\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.25899\n",
            "Epoch 10/30\n",
            " - 10s - loss: 1.3238 - acc: 0.4957 - val_loss: 2.3211 - val_acc: 0.1767\n",
            "\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.25899\n",
            "Epoch 11/30\n",
            " - 10s - loss: 1.2809 - acc: 0.5098 - val_loss: 2.1556 - val_acc: 0.1655\n",
            "\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.25899\n",
            "Epoch 12/30\n",
            " - 10s - loss: 1.2617 - acc: 0.5223 - val_loss: 2.5197 - val_acc: 0.1386\n",
            "\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.25899\n",
            "Epoch 13/30\n",
            " - 10s - loss: 1.2367 - acc: 0.5338 - val_loss: 2.3643 - val_acc: 0.1711\n",
            "\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.25899\n",
            "Epoch 14/30\n",
            " - 10s - loss: 1.2261 - acc: 0.5368 - val_loss: 2.4071 - val_acc: 0.1566\n",
            "\n",
            "\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0009000000427477062.\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.25899\n",
            "Epoch 15/30\n",
            " - 10s - loss: 1.1976 - acc: 0.5505 - val_loss: 2.2729 - val_acc: 0.1952\n",
            "\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.25899\n",
            "Epoch 16/30\n",
            " - 10s - loss: 1.1922 - acc: 0.5501 - val_loss: 2.9642 - val_acc: 0.1378\n",
            "\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.25899\n",
            "Epoch 17/30\n",
            " - 10s - loss: 1.1720 - acc: 0.5599 - val_loss: 2.6454 - val_acc: 0.1500\n",
            "\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.25899\n",
            "Epoch 18/30\n",
            " - 10s - loss: 1.1647 - acc: 0.5661 - val_loss: 2.0122 - val_acc: 0.2403\n",
            "\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.25899\n",
            "Epoch 19/30\n",
            " - 10s - loss: 1.1686 - acc: 0.5604 - val_loss: 1.4629 - val_acc: 0.4536\n",
            "\n",
            "\n",
            "Epoch 00019: val_acc improved from 0.25899 to 0.45361, saving model to model.h5\n",
            "Epoch 20/30\n",
            " - 10s - loss: 1.1469 - acc: 0.5697 - val_loss: 2.9073 - val_acc: 0.1436\n",
            "\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.45361\n",
            "Epoch 21/30\n",
            " - 10s - loss: 1.1363 - acc: 0.5745 - val_loss: 2.7032 - val_acc: 0.1429\n",
            "\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.45361\n",
            "Epoch 22/30\n",
            " - 10s - loss: 1.1217 - acc: 0.5811 - val_loss: 1.8255 - val_acc: 0.3391\n",
            "\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.45361\n",
            "Epoch 23/30\n",
            " - 10s - loss: 1.1070 - acc: 0.5887 - val_loss: 2.6067 - val_acc: 0.1902\n",
            "\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.45361\n",
            "Epoch 24/30\n",
            " - 10s - loss: 1.0949 - acc: 0.5932 - val_loss: 2.7374 - val_acc: 0.1545\n",
            "\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.45361\n",
            "Epoch 25/30\n",
            " - 10s - loss: 1.1398 - acc: 0.5746 - val_loss: 2.5735 - val_acc: 0.2366\n",
            "\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.45361\n",
            "Epoch 26/30\n",
            " - 10s - loss: 1.0952 - acc: 0.5965 - val_loss: 1.8471 - val_acc: 0.3675\n",
            "\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.45361\n",
            "Epoch 27/30\n",
            " - 10s - loss: 1.0806 - acc: 0.5979 - val_loss: 2.0281 - val_acc: 0.3533\n",
            "\n",
            "\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0008100000384729356.\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.45361\n",
            "Epoch 28/30\n",
            " - 10s - loss: 1.0745 - acc: 0.5999 - val_loss: 1.5190 - val_acc: 0.4366\n",
            "\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.45361\n",
            "Epoch 29/30\n",
            " - 10s - loss: 1.0545 - acc: 0.6089 - val_loss: 2.0226 - val_acc: 0.3413\n",
            "\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.45361\n",
            "Epoch 30/30\n",
            " - 10s - loss: 1.0440 - acc: 0.6127 - val_loss: 2.5622 - val_acc: 0.1822\n",
            "\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.45361\n",
            "legjobb val_acc:\n",
            "0.4536082474060728\n",
            "a modell hiperparaméterei: \n",
            "32\n",
            "0.38230833827442506\n",
            "0.20633103640535622\n",
            "tanh\n",
            "adam\n",
            "128\n",
            "Train on 25120 samples, validate on 7178 samples\n",
            "Epoch 1/30\n",
            " - 10s - loss: 1.9863 - acc: 0.2087 - val_loss: 1.7726 - val_acc: 0.2765\n",
            "\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.27654, saving model to model.h5\n",
            "Epoch 2/30\n",
            " - 6s - loss: 1.7909 - acc: 0.2869 - val_loss: 2.0697 - val_acc: 0.1856\n",
            "\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.27654\n",
            "Epoch 3/30\n",
            " - 6s - loss: 1.6290 - acc: 0.3755 - val_loss: 1.9827 - val_acc: 0.2133\n",
            "\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.27654\n",
            "Epoch 4/30\n",
            " - 6s - loss: 1.5572 - acc: 0.4035 - val_loss: 1.6863 - val_acc: 0.3207\n",
            "\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.27654 to 0.32070, saving model to model.h5\n",
            "Epoch 5/30\n",
            " - 6s - loss: 1.4991 - acc: 0.4254 - val_loss: 2.0932 - val_acc: 0.1712\n",
            "\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.32070\n",
            "Epoch 6/30\n",
            " - 6s - loss: 1.4718 - acc: 0.4339 - val_loss: 1.9700 - val_acc: 0.2216\n",
            "\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.32070\n",
            "Epoch 7/30\n",
            " - 6s - loss: 1.4331 - acc: 0.4570 - val_loss: 1.7022 - val_acc: 0.3079\n",
            "\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.32070\n",
            "Epoch 8/30\n",
            " - 6s - loss: 1.4121 - acc: 0.4649 - val_loss: 1.7795 - val_acc: 0.2712\n",
            "\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.32070\n",
            "Epoch 9/30\n",
            " - 6s - loss: 1.4026 - acc: 0.4692 - val_loss: 2.0008 - val_acc: 0.1925\n",
            "\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.32070\n",
            "Epoch 10/30\n",
            " - 6s - loss: 1.3812 - acc: 0.4771 - val_loss: 1.9809 - val_acc: 0.1989\n",
            "\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.32070\n",
            "Epoch 11/30\n",
            " - 6s - loss: 1.3558 - acc: 0.4884 - val_loss: 1.7278 - val_acc: 0.3805\n",
            "\n",
            "\n",
            "Epoch 00011: val_acc improved from 0.32070 to 0.38047, saving model to model.h5\n",
            "Epoch 12/30\n",
            " - 6s - loss: 1.3401 - acc: 0.4948 - val_loss: 1.6727 - val_acc: 0.3689\n",
            "\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.38047\n",
            "Epoch 13/30\n",
            " - 6s - loss: 1.3452 - acc: 0.4928 - val_loss: 1.5951 - val_acc: 0.3748\n",
            "\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.38047\n",
            "Epoch 14/30\n",
            " - 6s - loss: 1.3197 - acc: 0.5002 - val_loss: 1.5074 - val_acc: 0.4133\n",
            "\n",
            "\n",
            "Epoch 00014: val_acc improved from 0.38047 to 0.41335, saving model to model.h5\n",
            "Epoch 15/30\n",
            " - 6s - loss: 1.3250 - acc: 0.4990 - val_loss: 1.9798 - val_acc: 0.2625\n",
            "\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.41335\n",
            "Epoch 16/30\n",
            " - 6s - loss: 1.3173 - acc: 0.5047 - val_loss: 1.9587 - val_acc: 0.2437\n",
            "\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.41335\n",
            "Epoch 17/30\n",
            " - 6s - loss: 1.3063 - acc: 0.5105 - val_loss: 1.6637 - val_acc: 0.3764\n",
            "\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.41335\n",
            "Epoch 18/30\n",
            " - 6s - loss: 1.3300 - acc: 0.4981 - val_loss: 2.0863 - val_acc: 0.1587\n",
            "\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.41335\n",
            "Epoch 19/30\n",
            " - 6s - loss: 1.3180 - acc: 0.5043 - val_loss: 2.0429 - val_acc: 0.2480\n",
            "\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.41335\n",
            "Epoch 20/30\n",
            " - 6s - loss: 1.3184 - acc: 0.5019 - val_loss: 1.9595 - val_acc: 0.2267\n",
            "\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.41335\n",
            "Epoch 21/30\n",
            " - 6s - loss: 1.3023 - acc: 0.5115 - val_loss: 1.4638 - val_acc: 0.4308\n",
            "\n",
            "\n",
            "Epoch 00021: val_acc improved from 0.41335 to 0.43076, saving model to model.h5\n",
            "Epoch 22/30\n",
            " - 6s - loss: 1.2962 - acc: 0.5113 - val_loss: 1.4420 - val_acc: 0.4599\n",
            "\n",
            "\n",
            "Epoch 00022: val_acc improved from 0.43076 to 0.45988, saving model to model.h5\n",
            "Epoch 23/30\n",
            " - 6s - loss: 1.3012 - acc: 0.5105 - val_loss: 1.4867 - val_acc: 0.4097\n",
            "\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.45988\n",
            "Epoch 24/30\n",
            " - 6s - loss: 1.2840 - acc: 0.5198 - val_loss: 1.6581 - val_acc: 0.3628\n",
            "\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.45988\n",
            "Epoch 25/30\n",
            " - 6s - loss: 1.3005 - acc: 0.5104 - val_loss: 1.9249 - val_acc: 0.2754\n",
            "\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.45988\n",
            "Epoch 26/30\n",
            " - 6s - loss: 1.2847 - acc: 0.5159 - val_loss: 1.9569 - val_acc: 0.2417\n",
            "\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.45988\n",
            "Epoch 27/30\n",
            " - 6s - loss: 1.2823 - acc: 0.5181 - val_loss: 1.9514 - val_acc: 0.2395\n",
            "\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.45988\n",
            "Epoch 28/30\n",
            " - 6s - loss: 1.3067 - acc: 0.5048 - val_loss: 1.8058 - val_acc: 0.3502\n",
            "\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.45988\n",
            "Epoch 29/30\n",
            " - 6s - loss: 1.2835 - acc: 0.5202 - val_loss: 1.6983 - val_acc: 0.3729\n",
            "\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.45988\n",
            "Epoch 30/30\n",
            " - 6s - loss: 1.2660 - acc: 0.5298 - val_loss: 1.9098 - val_acc: 0.3147\n",
            "\n",
            "\n",
            "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0009000000427477062.\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.45988\n",
            "legjobb val_acc:\n",
            "0.45987740317637227\n",
            "a modell hiperparaméterei: \n",
            "64\n",
            "0.4482560975599062\n",
            "0.01661663926204826\n",
            "tanh\n",
            "rmsprop\n",
            "256\n",
            "Train on 25120 samples, validate on 7178 samples\n",
            "Epoch 1/30\n",
            " - 15s - loss: 2.2281 - acc: 0.1887 - val_loss: 2.1035 - val_acc: 0.1771\n",
            "\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.17707, saving model to model.h5\n",
            "Epoch 2/30\n",
            " - 10s - loss: 2.0327 - acc: 0.1988 - val_loss: 1.9159 - val_acc: 0.2502\n",
            "\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.17707 to 0.25021, saving model to model.h5\n",
            "Epoch 3/30\n",
            " - 10s - loss: 1.9033 - acc: 0.2293 - val_loss: 1.9985 - val_acc: 0.1450\n",
            "\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.25021\n",
            "Epoch 4/30\n",
            " - 10s - loss: 1.8122 - acc: 0.2685 - val_loss: 2.0142 - val_acc: 0.1977\n",
            "\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.25021\n",
            "Epoch 5/30\n",
            " - 10s - loss: 1.6886 - acc: 0.3396 - val_loss: 2.1077 - val_acc: 0.1771\n",
            "\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.25021\n",
            "Epoch 6/30\n",
            " - 10s - loss: 1.5980 - acc: 0.3778 - val_loss: 2.1960 - val_acc: 0.1771\n",
            "\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.25021\n",
            "Epoch 7/30\n",
            " - 10s - loss: 1.5197 - acc: 0.4131 - val_loss: 2.0847 - val_acc: 0.1714\n",
            "\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.25021\n",
            "Epoch 8/30\n",
            " - 10s - loss: 1.4747 - acc: 0.4334 - val_loss: 2.0716 - val_acc: 0.1712\n",
            "\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.25021\n",
            "Epoch 9/30\n",
            " - 10s - loss: 1.4304 - acc: 0.4496 - val_loss: 1.7918 - val_acc: 0.2863\n",
            "\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.25021 to 0.28629, saving model to model.h5\n",
            "Epoch 10/30\n",
            " - 10s - loss: 1.3963 - acc: 0.4643 - val_loss: 2.1326 - val_acc: 0.2097\n",
            "\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.28629\n",
            "Epoch 11/30\n",
            " - 10s - loss: 1.3603 - acc: 0.4791 - val_loss: 1.6283 - val_acc: 0.3781\n",
            "\n",
            "\n",
            "Epoch 00011: val_acc improved from 0.28629 to 0.37810, saving model to model.h5\n",
            "Epoch 12/30\n",
            " - 10s - loss: 1.3272 - acc: 0.4926 - val_loss: 2.2219 - val_acc: 0.1884\n",
            "\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.37810\n",
            "Epoch 13/30\n",
            " - 10s - loss: 1.3145 - acc: 0.4980 - val_loss: 2.3364 - val_acc: 0.1826\n",
            "\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.37810\n",
            "Epoch 14/30\n",
            " - 10s - loss: 1.2998 - acc: 0.5078 - val_loss: 1.7520 - val_acc: 0.3107\n",
            "\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.37810\n",
            "Epoch 15/30\n",
            " - 10s - loss: 1.2758 - acc: 0.5138 - val_loss: 2.0372 - val_acc: 0.2168\n",
            "\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.37810\n",
            "Epoch 16/30\n",
            " - 10s - loss: 1.2585 - acc: 0.5223 - val_loss: 2.6407 - val_acc: 0.1531\n",
            "\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.37810\n",
            "Epoch 17/30\n",
            " - 10s - loss: 1.2478 - acc: 0.5299 - val_loss: 2.3816 - val_acc: 0.1888\n",
            "\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.37810\n",
            "Epoch 18/30\n",
            " - 10s - loss: 1.2271 - acc: 0.5357 - val_loss: 2.3748 - val_acc: 0.1839\n",
            "\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.37810\n",
            "Epoch 19/30\n",
            " - 10s - loss: 1.2176 - acc: 0.5424 - val_loss: 2.4133 - val_acc: 0.1967\n",
            "\n",
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0009000000427477062.\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.37810\n",
            "Epoch 20/30\n",
            " - 10s - loss: 1.2004 - acc: 0.5493 - val_loss: 1.8972 - val_acc: 0.3233\n",
            "\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.37810\n",
            "Epoch 21/30\n",
            " - 10s - loss: 1.1831 - acc: 0.5558 - val_loss: 2.2598 - val_acc: 0.2053\n",
            "\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.37810\n",
            "Epoch 22/30\n",
            " - 10s - loss: 1.1807 - acc: 0.5551 - val_loss: 1.8025 - val_acc: 0.3022\n",
            "\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.37810\n",
            "Epoch 23/30\n",
            " - 10s - loss: 1.1693 - acc: 0.5607 - val_loss: 2.2528 - val_acc: 0.2091\n",
            "\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.37810\n",
            "Epoch 24/30\n",
            " - 10s - loss: 1.1594 - acc: 0.5694 - val_loss: 1.9747 - val_acc: 0.2803\n",
            "\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.37810\n",
            "Epoch 25/30\n",
            " - 10s - loss: 1.1481 - acc: 0.5699 - val_loss: 1.6863 - val_acc: 0.3456\n",
            "\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.37810\n",
            "Epoch 26/30\n",
            " - 10s - loss: 1.1484 - acc: 0.5709 - val_loss: 2.2043 - val_acc: 0.2014\n",
            "\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.37810\n",
            "legjobb val_acc:\n",
            "0.3780997492503774\n",
            "100%|██████████| 5/5 [22:23<00:00, 256.78s/it, best loss: -0.4991641125993896]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wPkbsP3T84tz",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}