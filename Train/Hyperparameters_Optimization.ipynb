{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "emotionrecopt.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "AK-YF0Xq7T7O",
        "i4Sa9Xpk7T7d",
        "yUpQXFku7T7j",
        "2-2OeSGt7T7o",
        "RO9rFd237T7t",
        "repC6M-g7T7x",
        "lQullSP27T71",
        "3OvKXj8Q7T75",
        "velppBYd7T8B",
        "NkbdS6xi7T8F",
        "YcSdRfJ47T8P",
        "o9LpI_wg7T8g",
        "SKJgXaTq7T8n",
        "98vZGUsWhRam",
        "DOw9DeK5hRar"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9V8mqxjS7T67",
        "colab": {}
      },
      "source": [
        "# Include all the necessary packages, \n",
        "import random\n",
        "import pandas as pd # pandas for data manipulation and analysis\n",
        "import numpy as np # numpy to handle multi-dimensional arrays and matricies\n",
        "import matplotlib # matplotlib for pixel plot\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JWFV9sr07T8o",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6f0UmeFH-hwb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "f033327b-7f0d-44a0-97c3-feadb4eab79d"
      },
      "source": [
        "#Include all the necessary keras libraries\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.losses import categorical_crossentropy\n",
        "from keras.optimizers import Adam\n",
        "from keras.regularizers import l2\n",
        "from keras.callbacks import ReduceLROnPlateau, TensorBoard, EarlyStopping, ModelCheckpoint\n",
        "from keras.models import load_model"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4LxbfouWhRZ-"
      },
      "source": [
        "# Hyperparameter optimalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P5yiuvd_84ti",
        "outputId": "910de32d-09a1-472b-81dd-3a81612b3c95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip3 install hyperas\n",
        "!pip3 install hyperopt"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting hyperas\n",
            "  Downloading https://files.pythonhosted.org/packages/04/34/87ad6ffb42df9c1fa9c4c906f65813d42ad70d68c66af4ffff048c228cd4/hyperas-0.4.1-py3-none-any.whl\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from hyperas) (5.6.1)\n",
            "Requirement already satisfied: jupyter in /usr/local/lib/python3.6/dist-packages (from hyperas) (1.0.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.6/dist-packages (from hyperas) (0.3)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from hyperas) (2.2.5)\n",
            "Requirement already satisfied: hyperopt in /usr/local/lib/python3.6/dist-packages (from hyperas) (0.1.2)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.6/dist-packages (from hyperas) (4.4.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (0.8.4)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (0.6.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (4.3.3)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (2.1.3)\n",
            "Requirement already satisfied: jinja2>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (2.10.3)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (1.4.2)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (4.6.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (3.1.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (0.4.4)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (4.6.0)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (4.6.1)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (7.5.1)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (5.2.0)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (5.2.2)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (1.17.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (2.8.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (1.3.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (3.13)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (1.12.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (1.0.8)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (3.9.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (0.16.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (2.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (4.28.1)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from nbformat->hyperas) (0.2.0)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat->hyperas) (2.6.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->nbconvert->hyperas) (4.4.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.4->nbconvert->hyperas) (1.1.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->hyperas) (0.5.1)\n",
            "Requirement already satisfied: jupyter-client>=4.1 in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->hyperas) (5.3.4)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->jupyter->hyperas) (4.5.3)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->jupyter->hyperas) (5.5.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->hyperas) (3.5.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->hyperas) (1.0.18)\n",
            "Requirement already satisfied: terminado>=0.3.3; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->hyperas) (0.8.3)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from jupyter-client>=4.1->qtconsole->jupyter->hyperas) (2.6.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client>=4.1->qtconsole->jupyter->hyperas) (17.0.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->hyperas) (0.8.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->hyperas) (4.7.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->hyperas) (42.0.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->hyperas) (0.7.5)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.0->jupyter-console->jupyter->hyperas) (0.1.7)\n",
            "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.3.3; sys_platform != \"win32\"->notebook->jupyter->hyperas) (0.6.0)\n",
            "Installing collected packages: hyperas\n",
            "Successfully installed hyperas-0.4.1\n",
            "Requirement already satisfied: hyperopt in /usr/local/lib/python3.6/dist-packages (0.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from hyperopt) (1.12.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from hyperopt) (4.28.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from hyperopt) (1.3.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from hyperopt) (1.17.4)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt) (3.9.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt) (2.4)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->hyperopt) (4.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3ZDv2Ry984tk",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.callbacks import EarlyStopping\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tzdPnsGJ84tm",
        "colab": {}
      },
      "source": [
        "def data():\n",
        "  df = pd.read_csv('fer2013.csv')\n",
        "  # Convert the pixel column values into a Python list object\n",
        "  pixels = df['pixels'].tolist()\n",
        "  emotions = df['emotion'].tolist()\n",
        "\n",
        "  # The fer2013 database contains 48x48 face images so we create two variables \n",
        "  # to store the width and the height of the image\n",
        "  width, height = 48, 48\n",
        "\n",
        "  # Convert each pixel set (pixel array) to a 48x48 image and\n",
        "  # create a list called faces to store each face image\n",
        "  faces = []\n",
        "  for pixel_sequence in pixels:\n",
        "      # Use Python's list comprehension because it's quicker than a single for cycle\n",
        "      face = [int(pixel) for pixel in pixel_sequence.split(' ')]\n",
        "      # Reshape face array to matches the 48x48 face images\n",
        "      face = np.asarray(face).reshape(width, height)\n",
        "      # Add the converted face image to the faces list\n",
        "      faces.append(face.astype('uint8'))\n",
        "\n",
        "  # Convert the list to a numpy array\n",
        "  faces = np.asarray(faces)\n",
        "  emotions = np.asarray(emotions)\n",
        "\n",
        "  # Expanding the dimension of channel for each image\n",
        "  faces_exp = np.expand_dims(faces, -1)\n",
        "  # Converting the labels to catergorical matrix\n",
        "  emotions_categori = pd.get_dummies(df['emotion']).as_matrix() \n",
        "\n",
        "  # Create a dictionary for identify the emotion\n",
        "  emotion_dict = {0:\"Angry\", 1:\"Disgust\", 2:\"Fear\", 3:\"Happy\", 4:\"Sad\", 5:\"Surprise\", 6:\"Neutral\"}\n",
        "\n",
        "  # Split the train data into a train and validation group (validation = 20% of the train data)\n",
        "  x_train, x_val, y_train, y_val = train_test_split(faces_exp, emotions_categori, test_size=0.2, random_state=30)\n",
        "  # Split the train data into a train and test group (test = 10% of the original train data)\n",
        "  x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.125, random_state=30)\n",
        "\n",
        "  return x_train, y_train, x_test, y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ytq3gBgS84to",
        "colab": {}
      },
      "source": [
        "def create_model(x_train,y_train,x_test,y_test):\n",
        "    from keras.layers import Layer\n",
        "    from keras import backend as K\n",
        "    from keras.layers import LeakyReLU\n",
        "\n",
        "    num_features = 64 \n",
        "    num_labels = 7 # Seven different emotions\n",
        "    batch_size = 64 # One batch contains 64 images\n",
        "    epochs = 1\n",
        "    width, height = 48, 48  # Image size\n",
        " \n",
        "    n_layer = {{choice([16, 32, 64])}}\n",
        "    dropout_n = {{uniform(0, 0.5)}}\n",
        "    dropout_2 = {{uniform(0, 0.5)}}\n",
        "    act = {{choice(['relu', 'linear', 'tanh'])}}\n",
        "    optim = {{choice(['rmsprop', 'adam', 'sgd'])}}\n",
        "    n_batch = {{choice([64, 128, 256])}}\n",
        "    print('a modell hiperparaméterei: ', n_layer, dropout_n, dropout_2, act, optim, n_batch)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(n_layer, kernel_size=(3, 3), activation = act, input_shape=(width, height, 1), data_format='channels_last', kernel_regularizer=l2(0.01)))\n",
        "    model.add(Conv2D(n_layer, kernel_size=(3, 3), activation = act, padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "    model.add(Dropout(dropout_n))\n",
        "\n",
        "    model.add(Conv2D(2*n_layer, kernel_size=(3, 3), activation = act, padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(2*n_layer, kernel_size=(3, 3), activation = act, padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    model.add(Conv2D(2*2*n_layer, kernel_size=(3, 3), activation = act, padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(2*2*n_layer, kernel_size=(3, 3), activation = act, padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "    model.add(Dropout(dropout_n))\n",
        "\n",
        "    model.add(Conv2D(2*2*2*n_layer, kernel_size=(3, 3), activation = act, padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(2*2*2*n_layer, kernel_size=(3, 3), activation = act, padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "    model.add(Dropout(dropout_n))\n",
        "\n",
        "    # Transform the Inputs to row vector\n",
        "    model.add(Flatten())\n",
        "              \n",
        "    # With dense, we define the fully-connected layers \n",
        "    model.add(Dense(2*2*2*n_layer, activation = act))\n",
        "    model.add(Dropout(dropout_n))\n",
        "    model.add(Dense(2*2*n_layer, activation = act))\n",
        "    model.add(Dropout(dropout_n))\n",
        "    model.add(Dense(2*n_layer, activation = act))\n",
        "    model.add(Dropout(dropout_n))\n",
        "    \n",
        "    model.add(Dense(num_labels, activation='softmax'))\n",
        "\n",
        "    model.compile(loss = categorical_crossentropy,\n",
        "                  optimizer = optim,\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    lr_reducer = ReduceLROnPlateau(monitor='val_acc', factor=0.9, patience=3, verbose=1)\n",
        "\n",
        "    early_stopper = EarlyStopping(monitor='val_acc', min_delta=0, patience=3, verbose=0, mode='auto')\n",
        "\n",
        "    checkpointer = ModelCheckpoint('model.h5', monitor='val_acc', verbose=1, save_best_only=True)\n",
        "\n",
        "    result = model.fit(np.array(x_train), np.array(y_train),\n",
        "              batch_size = n_batch,\n",
        "              epochs = epochs,\n",
        "              verbose = 2,\n",
        "              validation_data = (np.array(x_val), np.array(y_val)),\n",
        "              shuffle = True,\n",
        "              callbacks = [lr_reducer, early_stopper, checkpointer])\n",
        "\n",
        "    \n",
        "    # az epoch-ok közül a legnagyobb val_acc elmentése\n",
        "    best_val_acc = np.amax(result.history['val_acc']) \n",
        "    print('legjobb val_acc:', best_val_acc)  \n",
        "    \n",
        "    # log kiírása: háló struktúra, és az eredmény\n",
        "    with open('emotionrecopt.csv', 'a') as csv_file:\n",
        "      csv_file.write(str(n_layer) + ';')\n",
        "      csv_file.write(str(dropout_n) + ';')\n",
        "      csv_file.write(str(act) + ';')\n",
        "      csv_file.write(str(optim) + ';')\n",
        "      csv_file.write(str(n_batch) + ';')\n",
        "      csv_file.write(str(best_val_acc) + '\\n')\n",
        "\n",
        "\n",
        "    # negatív val_acc, mert a hyperopt csomag mindig minimalizál\n",
        "    return {'loss': -best_val_acc, 'status': STATUS_OK, 'model': model}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bavDuAKi84tr",
        "colab": {}
      },
      "source": [
        "import hyperas\n",
        "from hyperopt import Trials, STATUS_OK, tpe\n",
        "from hyperas import optim\n",
        "from hyperas.distributions import choice,uniform"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CCVSxxV284tv",
        "outputId": "183808dc-8bba-403f-9e73-b4389443b0ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "best_run, best_model = optim.minimize(model=create_model,\n",
        "                                          data=data,\n",
        "                                          algo=tpe.suggest,\n",
        "                                          max_evals=5,\n",
        "                                          notebook_name='emotionrecopt',\n",
        "                                          trials=Trials())"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">>> Imports:\n",
            "#coding=utf-8\n",
            "\n",
            "try:\n",
            "    import random\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import pandas as pd\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import numpy as np\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import matplotlib\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import matplotlib.pyplot as plt\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from sklearn.model_selection import train_test_split\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.models import Sequential\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.layers import Dense, Dropout, Activation, Flatten\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.losses import categorical_crossentropy\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.optimizers import Adam\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.regularizers import l2\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.callbacks import ReduceLROnPlateau, TensorBoard, EarlyStopping, ModelCheckpoint\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.models import load_model\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import keras\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.datasets import fashion_mnist\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.models import Sequential\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.layers import Dense, Dropout, Activation\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.callbacks import EarlyStopping\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import numpy as np\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.layers import Layer\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras import backend as K\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.layers import LeakyReLU\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import hyperas\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from hyperopt import Trials, STATUS_OK, tpe\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from hyperas import optim\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from hyperas.distributions import choice, uniform\n",
            "except:\n",
            "    pass\n",
            "\n",
            ">>> Hyperas search space:\n",
            "\n",
            "def get_space():\n",
            "    return {\n",
            "        'n_layer': hp.choice('n_layer', [16, 32, 64]),\n",
            "        'dropout_n': hp.uniform('dropout_n', 0, 0.5),\n",
            "        'dropout_n_1': hp.uniform('dropout_n_1', 0, 0.5),\n",
            "        'act': hp.choice('act', ['relu', 'linear', 'tanh']),\n",
            "        'optim': hp.choice('optim', ['rmsprop', 'adam', 'sgd']),\n",
            "        'n_batch': hp.choice('n_batch', [64, 128, 256]),\n",
            "    }\n",
            "\n",
            ">>> Data\n",
            "   1: \n",
            "   2: df = pd.read_csv('fer2013.csv')\n",
            "   3: # Convert the pixel column values into a Python list object\n",
            "   4: pixels = df['pixels'].tolist()\n",
            "   5: emotions = df['emotion'].tolist()\n",
            "   6: \n",
            "   7: # The fer2013 database contains 48x48 face images so we create two variables \n",
            "   8: # to store the width and the height of the image\n",
            "   9: width, height = 48, 48\n",
            "  10: \n",
            "  11: # Convert each pixel set (pixel array) to a 48x48 image and\n",
            "  12: # create a list called faces to store each face image\n",
            "  13: faces = []\n",
            "  14: for pixel_sequence in pixels:\n",
            "  15:     # Use Python's list comprehension because it's quicker than a single for cycle\n",
            "  16:     face = [int(pixel) for pixel in pixel_sequence.split(' ')]\n",
            "  17:     # Reshape face array to matches the 48x48 face images\n",
            "  18:     face = np.asarray(face).reshape(width, height)\n",
            "  19:     # Add the converted face image to the faces list\n",
            "  20:     faces.append(face.astype('uint8'))\n",
            "  21: \n",
            "  22: # Convert the list to a numpy array\n",
            "  23: faces = np.asarray(faces)\n",
            "  24: emotions = np.asarray(emotions)\n",
            "  25: \n",
            "  26: # Expanding the dimension of channel for each image\n",
            "  27: faces_exp = np.expand_dims(faces, -1)\n",
            "  28: # Converting the labels to catergorical matrix\n",
            "  29: emotions_categori = pd.get_dummies(df['emotion']).as_matrix() \n",
            "  30: \n",
            "  31: # Create a dictionary for identify the emotion\n",
            "  32: emotion_dict = {0:\"Angry\", 1:\"Disgust\", 2:\"Fear\", 3:\"Happy\", 4:\"Sad\", 5:\"Surprise\", 6:\"Neutral\"}\n",
            "  33: \n",
            "  34: #faces_full = np.concatenate((faces, transformed_faces))\n",
            "  35: #emotions_full = np.concatenate((emotions, transformed_emotions))\n",
            "  36: # After the equalization, we make the new databse\n",
            "  37: # augmented_df = pd.DataFrame()           \n",
            "  38: # for i in range(len(emotions_full)):\n",
            "  39:   # row = pd.Series([emotions_full[i], faces_full[i]], index=[\"emotion\", \"pixels\"], name=str(i))\n",
            "  40:   # augmented_df = augmented_df.append(row)\n",
            "  41: \n",
            "  42: # pixels = augmented_df['pixels'].tolist()\n",
            "  43: \n",
            "  44: # Convert the list to a numpy array\n",
            "  45: # faces = np.asarray(pixels)\n",
            "  46: # Expanding the dimension of channel for each image\n",
            "  47: # faces_full_expand = np.expand_dims(faces, -1)\n",
            "  48: # Converting the labels to catergorical matrix\n",
            "  49: # emotions_categori = pd.get_dummies(augmented_df['emotion']).as_matrix() \n",
            "  50: \n",
            "  51: # Split the train data into a train and validation group (validation = 20% of the train data)\n",
            "  52: x_train, x_val, y_train, y_val = train_test_split(faces_exp, emotions_categori, test_size=0.2, random_state=30)\n",
            "  53: # Split the train data into a train and test group (test = 10% of the original train data)\n",
            "  54: x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.125, random_state=30)\n",
            "  55: \n",
            "  56: \n",
            "  57: \n",
            "  58: \n",
            ">>> Resulting replaced keras model:\n",
            "\n",
            "   1: def keras_fmin_fnct(space):\n",
            "   2: \n",
            "   3: \n",
            "   4:     num_features = 64 \n",
            "   5:     num_labels = 7 # Seven different emotions\n",
            "   6:     batch_size = 64 # One batch contains 64 images\n",
            "   7:     epochs = 1\n",
            "   8:     width, height = 48, 48  # Image size\n",
            "   9:     class Swish(Layer):\n",
            "  10:         def __init__(self, beta, **kwargs):\n",
            "  11:             super(Swish, self).__init__(**kwargs)\n",
            "  12:             self.beta = K.cast_to_floatx(beta)\n",
            "  13: \n",
            "  14:         def call(self, inputs):\n",
            "  15:             return K.sigmoid(self.beta * inputs) * inputs\n",
            "  16: \n",
            "  17:         def get_config(self):\n",
            "  18:             config = {'beta': float(self.beta)}\n",
            "  19:             base_config = super(Swish, self).get_config()\n",
            "  20:             return dict(list(base_config.items()) + list(config.items()))\n",
            "  21: \n",
            "  22:         def compute_output_shape(self, input_shape):\n",
            "  23:             return input_shape\n",
            "  24:     # eddig tart a swish definíció\n",
            "  25:  \n",
            "  26:     n_layer = space['n_layer']\n",
            "  27:     dropout_n = space['dropout_n']\n",
            "  28:     dropout_2 = space['dropout_n_1']\n",
            "  29:     act = space['act']\n",
            "  30:     optim = space['optim']\n",
            "  31:     n_batch = space['n_batch']\n",
            "  32:     print('a modell hiperparaméterei: ', n_layer, dropout_n, dropout_2, act, optim, n_batch)\n",
            "  33: \n",
            "  34:     model = Sequential()\n",
            "  35:     # Firsrt we defined the conv layers\n",
            "  36:     # All layers have RELU activation function\n",
            "  37:     # First layer has L2 regularization\n",
            "  38:     model.add(Conv2D(n_layer, kernel_size=(3, 3), activation = act, input_shape=(width, height, 1), data_format='channels_last', kernel_regularizer=l2(0.01)))\n",
            "  39:     model.add(Conv2D(n_layer, kernel_size=(3, 3), activation = act, padding='same'))\n",
            "  40:     model.add(BatchNormalization())\n",
            "  41:     model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
            "  42:     model.add(Dropout(dropout_n))\n",
            "  43: \n",
            "  44:     model.add(Conv2D(2*n_layer, kernel_size=(3, 3), activation = act, padding='same'))\n",
            "  45:     model.add(BatchNormalization())\n",
            "  46:     model.add(Conv2D(2*n_layer, kernel_size=(3, 3), activation = act, padding='same'))\n",
            "  47:     model.add(BatchNormalization())\n",
            "  48:     model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
            "  49:     model.add(Dropout(0.5))\n",
            "  50: \n",
            "  51:     model.add(Conv2D(2*2*n_layer, kernel_size=(3, 3), activation = act, padding='same'))\n",
            "  52:     model.add(BatchNormalization())\n",
            "  53:     model.add(Conv2D(2*2*n_layer, kernel_size=(3, 3), activation = act, padding='same'))\n",
            "  54:     model.add(BatchNormalization())\n",
            "  55:     model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
            "  56:     model.add(Dropout(dropout_n))\n",
            "  57: \n",
            "  58:     model.add(Conv2D(2*2*2*n_layer, kernel_size=(3, 3), activation = act, padding='same'))\n",
            "  59:     model.add(BatchNormalization())\n",
            "  60:     model.add(Conv2D(2*2*2*n_layer, kernel_size=(3, 3), activation = act, padding='same'))\n",
            "  61:     model.add(BatchNormalization())\n",
            "  62:     model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
            "  63:     model.add(Dropout(dropout_n))\n",
            "  64: \n",
            "  65:     # Transform the Inputs to row vector\n",
            "  66:     model.add(Flatten())\n",
            "  67:               \n",
            "  68:     # With dense, we define the fully-connected layers \n",
            "  69:     model.add(Dense(2*2*2*n_layer, activation = act))\n",
            "  70:     model.add(Dropout(dropout_n))\n",
            "  71:     model.add(Dense(2*2*n_layer, activation = act))\n",
            "  72:     model.add(Dropout(dropout_n))\n",
            "  73:     model.add(Dense(2*n_layer, activation = act))\n",
            "  74:     model.add(Dropout(dropout_n))\n",
            "  75:     \n",
            "  76:     model.add(Dense(num_labels, activation='softmax'))\n",
            "  77: \n",
            "  78:     model.compile(loss = categorical_crossentropy,\n",
            "  79:                   optimizer = optim,\n",
            "  80:                   metrics=['accuracy'])\n",
            "  81: \n",
            "  82:     lr_reducer = ReduceLROnPlateau(monitor='val_acc', factor=0.9, patience=3, verbose=1)\n",
            "  83: \n",
            "  84:     early_stopper = EarlyStopping(monitor='val_acc', min_delta=0, patience=3, verbose=0, mode='auto')\n",
            "  85: \n",
            "  86:     checkpointer = ModelCheckpoint('model.h5', monitor='val_acc', verbose=1, save_best_only=True)\n",
            "  87: \n",
            "  88:     result = model.fit(np.array(x_train), np.array(y_train),\n",
            "  89:               batch_size = n_batch,\n",
            "  90:               epochs = epochs,\n",
            "  91:               verbose = 2,\n",
            "  92:               validation_data = (np.array(x_val), np.array(y_val)),\n",
            "  93:               shuffle = True,\n",
            "  94:               callbacks = [lr_reducer, early_stopper, checkpointer])\n",
            "  95: \n",
            "  96:     \n",
            "  97:     # az epoch-ok közül a legnagyobb val_acc elmentése\n",
            "  98:     best_val_acc = np.amax(result.history['val_acc']) \n",
            "  99:     print('legjobb val_acc:', best_val_acc)  \n",
            " 100:     \n",
            " 101:     # log kiírása: háló struktúra, és az eredmény\n",
            " 102:     with open('emotionrecopt.csv', 'a') as csv_file:\n",
            " 103:       csv_file.write(str(n_layer) + ';')\n",
            " 104:       csv_file.write(str(dropout_n) + ';')\n",
            " 105:       csv_file.write(str(act) + ';')\n",
            " 106:       csv_file.write(str(optim) + ';')\n",
            " 107:       csv_file.write(str(n_batch) + ';')\n",
            " 108:       csv_file.write(str(best_val_acc) + '\\n')\n",
            " 109: \n",
            " 110: \n",
            " 111:     # negatív val_acc, mert a hyperopt csomag mindig minimalizál\n",
            " 112:     return {'loss': -best_val_acc, 'status': STATUS_OK, 'model': model}\n",
            " 113: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/hyperas/utils.py:149: UserWarning: Inconsistent indentation detected.Found \"  \" (length: 2) as well as \"    \" (length: 4)\n",
            "  indent, len(indent), new_indent, len(new_indent)))\n",
            "/usr/local/lib/python3.6/dist-packages/hyperas/utils.py:149: UserWarning: Inconsistent indentation detected.Found \"    \" (length: 4) as well as \"  \" (length: 2)\n",
            "  indent, len(indent), new_indent, len(new_indent)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "a modell hiperparaméterei: \n",
            "32\n",
            "0.3671073489296299\n",
            "0.3462695171578595\n",
            "linear\n",
            "sgd\n",
            "64\n",
            "  0%|          | 0/5 [00:00<?, ?it/s, best loss: ?]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/temp_model.py:166: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
            "  emotions_categori = pd.get_dummies(df['emotion']).as_matrix()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 25120 samples, validate on 7178 samples\n",
            "Epoch 1/1\n",
            " - 18s - loss: 2.8561 - acc: 0.1959 - val_loss: 1.8101 - val_acc: 0.2487\n",
            "\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.24868, saving model to model.h5\n",
            "legjobb val_acc:\n",
            "0.2486765115631095\n",
            "a modell hiperparaméterei: \n",
            "64\n",
            "0.25764043248072455\n",
            "0.21261430843422813\n",
            "tanh\n",
            "rmsprop\n",
            "128\n",
            "Train on 25120 samples, validate on 7178 samples\n",
            "Epoch 1/1\n",
            " - 22s - loss: 2.0620 - acc: 0.1928 - val_loss: 1.9509 - val_acc: 0.1712\n",
            "\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.17122, saving model to model.h5\n",
            "legjobb val_acc:\n",
            "0.1712176093640152\n",
            "a modell hiperparaméterei: \n",
            "64\n",
            "0.23634271098298437\n",
            "0.3637790828042422\n",
            "tanh\n",
            "adam\n",
            "256\n",
            "Train on 25120 samples, validate on 7178 samples\n",
            "Epoch 1/1\n",
            " - 23s - loss: 2.0426 - acc: 0.1917 - val_loss: 1.8361 - val_acc: 0.1712\n",
            "\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.17122, saving model to model.h5\n",
            "legjobb val_acc:\n",
            "0.1712176093640152\n",
            "a modell hiperparaméterei: \n",
            "32\n",
            "0.38230833827442506\n",
            "0.20633103640535622\n",
            "tanh\n",
            "adam\n",
            "128\n",
            "Train on 25120 samples, validate on 7178 samples\n",
            "Epoch 1/1\n",
            " - 18s - loss: 2.0503 - acc: 0.1906 - val_loss: 1.8337 - val_acc: 0.2458\n",
            "\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.24575, saving model to model.h5\n",
            "legjobb val_acc:\n",
            "0.24575090554471998\n",
            "a modell hiperparaméterei: \n",
            "64\n",
            "0.4482560975599062\n",
            "0.01661663926204826\n",
            "tanh\n",
            "rmsprop\n",
            "256\n",
            "Train on 25120 samples, validate on 7178 samples\n",
            "Epoch 1/1\n",
            " - 22s - loss: 2.2145 - acc: 0.1829 - val_loss: 1.9304 - val_acc: 0.2458\n",
            "\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.24575, saving model to model.h5\n",
            "legjobb val_acc:\n",
            "0.24575090554471998\n",
            "100%|██████████| 5/5 [04:34<00:00, 55.05s/it, best loss: -0.2486765115631095]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wPkbsP3T84tz",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}